{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c74365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ivon-opt in /Users/nourjamoussi/anaconda3/lib/python3.11/site-packages (0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ivon-opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "223b6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import ivon\n",
    "from torch import Tensor\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1ce95",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad71e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclass of IVON to compute sampling using the covariance\n",
    "class IVON_SAMP(ivon.IVON):\n",
    "    # A subclass of IVON that rewrites _sample_params\n",
    "    # To make the sampling with a given covariance instead of the ess and the hess parameters in the original verison used to compute the covariance\n",
    "    def __init__(self, *args, cov, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        for group in self.param_groups:\n",
    "            group[\"cov\"] = cov\n",
    "    def _sample_params(self) -> Tuple[Tensor, Tensor]:\n",
    "            noise_samples = []\n",
    "            param_avgs = []\n",
    "            offset = 0\n",
    "            for group in self.param_groups:\n",
    "                gnumel = group[\"numel\"]\n",
    "                #noise_sample = (\n",
    "                #    torch.randn(gnumel, device=self._device, dtype=self._dtype)\n",
    "                #    / (\n",
    "                #        group[\"ess\"] * (group[\"hess\"] + group[\"weight_decay\"])\n",
    "                #    ).sqrt()\n",
    "                #)\n",
    "                cov = group[\"cov\"].astype(np.float32)\n",
    "                noise_sample = (\n",
    "                    torch.randn(gnumel, device=self._device, dtype=torch.float32)\n",
    "                    * torch.from_numpy(np.sqrt(cov)).to(self._device)\n",
    "                )\n",
    "\n",
    "                noise_samples.append(noise_sample)\n",
    "\n",
    "                goffset = 0\n",
    "                for p in group[\"params\"]:\n",
    "                    if p is None:\n",
    "                        continue\n",
    "\n",
    "                    p_avg = p.data.flatten()\n",
    "                    numel = p.numel()\n",
    "                    p_noise = noise_sample[offset : offset + numel]\n",
    "\n",
    "                    param_avgs.append(p_avg)\n",
    "                    p.data = (p_avg + p_noise).view(p.shape)\n",
    "                    goffset += numel\n",
    "                    offset += numel\n",
    "                assert goffset == group[\"numel\"]  # sanity check\n",
    "            assert offset == self._numel  # sanity check\n",
    "\n",
    "            return torch.cat(param_avgs, 0), torch.cat(noise_samples, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5743da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation functions\n",
    "def eaa(means, covs, weights=None): #Empirical Arithmetic Aggregation\n",
    "    mu = np.average(means, weights=weights, axis=0)\n",
    "    cov = np.average(covs, weights=weights, axis=0)\n",
    "    return mu, cov\n",
    "\n",
    "def gaa(means, covs, weights=None): #Gaussian Arithmetic Aggregation\n",
    "    mu = np.average(means, weights=weights, axis=0)\n",
    "    weights_squared = [weight**2 for weight in weights]\n",
    "    cov = np.average(covs, weights=weights_squared, axis=0)\n",
    "    return mu, cov\n",
    "\n",
    "def aalv(means, covs, weights=None): #Arithmetic Aggregation with Log Variance\n",
    "    mu = np.average(means, weights=weights, axis=0)\n",
    "    cov = np.exp(np.average(np.log(covs), weights=weights, axis=0))\n",
    "    return mu, cov\n",
    "\n",
    "\n",
    "\n",
    "def forward_kl_barycenter(means, covs, weights=None):\n",
    "    mu = np.average(means, weights=weights, axis=0)\n",
    "    toavg = covs + (means - mu)**2\n",
    "    cov = np.average(toavg, weights=weights, axis=0)\n",
    "    return mu, cov\n",
    "\n",
    "def reverse_kl_barycenter(means, covs, weights=None):  # Kullback Leiber Average\n",
    "    inverted_covs = [1/cov for cov in covs]\n",
    "    cov = 1/(np.average(inverted_covs, weights=weights, axis=0))\n",
    "    inverted_covs_time_means = [(inverted_covs[i] * means[i]).reshape(-1,1) for i in range(len(means))]\n",
    "    cov = cov.reshape(-1,1)\n",
    "    mu = cov * np.average(inverted_covs_time_means, weights=weights, axis=0)\n",
    "    return mu, cov\n",
    "\n",
    "\n",
    "def wasserstein_barycenter_diag(means, covs, weights = None):\n",
    "    #check if matrices are diagonal\n",
    "    assert all([ np.sum(K.cpu() > 1e-10) == K.cpu().shape[0] for K in covs]), \\\n",
    "        \"NotDiagonal: One of the covariance matrices is not diagonal.\"\n",
    "    #mu = np.average(means, weights=weights, axis=0)\n",
    "    mu = np.average(means.cpu(), weights=weights, axis=0)\n",
    "    cov = np.average([np.sqrt(K.cpu()) for K in covs], weights=weights, axis=0)**2  # for ivon\n",
    "    #cov = np.average([np.sqrt(K) for K in covs], weights=weights, axis=0)**2  #for non ivon\n",
    "    return mu, cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd09735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils for aggregation\n",
    "\n",
    "def flatten_model_state_dict(state_dict):\n",
    "    vec = []\n",
    "    for param_tensor in state_dict:\n",
    "        vec.append(state_dict[param_tensor].view(-1))\n",
    "    return torch.cat(vec)\n",
    "\n",
    "def unflatten_model_state_dict(vec, state_dict):\n",
    "    state_dict = state_dict.copy()\n",
    "    idx = 0\n",
    "    for param_tensor in state_dict:\n",
    "        param = state_dict[param_tensor]\n",
    "        size = param.numel()\n",
    "        state_dict[param_tensor] = torch.from_numpy( vec[idx:idx+size].reshape(param.size()) )\n",
    "        idx += size\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def aggregate_param(means, covs, method, weights=None):\n",
    "    means = means.cpu()\n",
    "    covs = covs.cpu()\n",
    "    if method == 'eaa':\n",
    "        mu , cov = eaa(means, covs, weights) # works with the diagonals of the covariance matrices\n",
    "        return mu, cov\n",
    "    elif method == 'wb_diag':\n",
    "        mu , cov = wasserstein_barycenter_diag(means, covs, weights)\n",
    "        return mu, cov\n",
    "    elif method == 'fkl':\n",
    "        mu , cov = forward_kl_barycenter(means, covs, weights)\n",
    "        return mu, cov\n",
    "    elif method == 'rkl':\n",
    "        mu , cov = reverse_kl_barycenter(means, covs, weights)\n",
    "        return mu, cov\n",
    "    elif method == 'gaa':\n",
    "        mu , cov = gaa(means, covs, weights)\n",
    "        return mu, cov\n",
    "    elif method == 'aalv':\n",
    "        mu , cov = aalv(means, covs, weights)\n",
    "        return mu, cov\n",
    "    else:\n",
    "        raise ValueError(f\"update method {method} non implemented!\")\n",
    "\n",
    "\n",
    "def aggregate_ivon(means, covs, update_method, global_model, weights=None):\n",
    "    means = [flatten_model_state_dict(m) for m in means]\n",
    "    means = torch.stack(means)\n",
    "    covs = torch.stack(covs)\n",
    "    mu_agg , cov_agg = aggregate_param(means, covs, update_method, weights)\n",
    "    agg_state_dict = unflatten_model_state_dict(mu_agg, global_model.state_dict())\n",
    "    return agg_state_dict , cov_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59147764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores of evaluation\n",
    "def ece(preds, target, device, minibatch=True):\n",
    "    confidences, predictions = torch.max(preds, 1)\n",
    "    target = target.float() # to avoid a warning error\n",
    "    _, target_cls = torch.max(target, 1)\n",
    "    accuracies = predictions.eq(target_cls)\n",
    "    n_bins = 100\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = torch.zeros(1, device=device)\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "        prop_in_bin = in_bin.float().mean()\n",
    "        if prop_in_bin.item() > 0:\n",
    "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "    return ece.item()\n",
    "\n",
    "def nll(preds, target, minibatch=True):\n",
    "    logpred = torch.log(preds + 1e-8)\n",
    "    if minibatch:\n",
    "        return -(logpred * target).sum(1).item()\n",
    "    else:\n",
    "        return -(logpred * target).sum(1).mean().item()\n",
    "\n",
    "def acc(preds, target, minibatch=True):\n",
    "    preds = preds.float() #to avoid a warning error\n",
    "    preds = preds.argmax(1)\n",
    "    target = target.float() # to avoid a warning error\n",
    "    target = target.argmax(1)\n",
    "    if minibatch:\n",
    "        return (((preds == target) * 1.0).sum() * 100).item()\n",
    "    else:\n",
    "        return (((preds == target) * 1.0).mean() * 100).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fea4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=10, input_channel=3):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channel, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.input_dim)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edd85f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "NUM_CLIENTS = 10\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.1\n",
    "REG = 1e-4\n",
    "DEVICE = 'mps'\n",
    "LOGDIR = './logs'\n",
    "N_SAMPLE = 0\n",
    "HESS_INIT = 0.1\n",
    "\n",
    "input_channel = 1\n",
    "input_dim = (16 * 4 * 4)\n",
    "hidden_dims=[120, 84]\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1473d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for local client\n",
    "def train_ivon(model, dataloader, criterion, optimizer, epochs, client_id, train_samples=3):\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.01)\n",
    "    model = model.to(DEVICE)\n",
    "    for _ in range(epochs):\n",
    "        for x, target in dataloader:\n",
    "            x, target = x.to(DEVICE), target.to(DEVICE)\n",
    "            for _ in range(train_samples):\n",
    "                with optimizer.sampled_params(train=True):\n",
    "                    logit = model(x)\n",
    "                    loss = criterion(logit, target)\n",
    "                    loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "    torch.save(optimizer.state_dict(), f\"{LOGDIR}/clients/client_{client_id}_optimizer.pt\")\n",
    "    return model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5842eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data_statistics \n",
    "with open('train_data_statistics_0.pkl', 'rb') as f:\n",
    "    train_data_statistics = pickle.load(f)\n",
    "\n",
    "# Generate a uniform split dictionary (if needed)\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def generate_uniform_split(num_clients=10, num_classes=10, total_samples=60000):\n",
    "#     samples_per_client = total_samples // num_clients\n",
    "#     samples_per_class_per_client = samples_per_client // num_classes\n",
    "\n",
    "#     split_dict = defaultdict(dict)\n",
    "#     for client_id in range(num_clients):\n",
    "#         for class_id in range(num_classes):\n",
    "#             split_dict[client_id][class_id] = samples_per_class_per_client\n",
    "\n",
    "#     return split_dict\n",
    "# split_dict = generate_uniform_split(num_clients=NUM_CLIENTS, num_classes=10, total_samples=len(full_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6e793a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data from the statistics\n",
    "def create_client_datasets_from_split(full_dataset, split_dict):\n",
    "    # 1. Organize all indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(full_dataset):\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    # 2. Shuffle class indices for randomness\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "\n",
    "    # 3. Build each client dataset\n",
    "    client_datasets = []\n",
    "    class_counters = {c: 0 for c in range(10)}  # track used indices per class\n",
    "\n",
    "    for client_id in sorted(split_dict.keys()):\n",
    "        client_indices = []\n",
    "        for class_label, count in split_dict[client_id].items():\n",
    "            start = class_counters[class_label]\n",
    "            end = start + count\n",
    "            client_indices.extend(class_indices[class_label][start:end])\n",
    "            class_counters[class_label] = end  # update the counter\n",
    "        # Append subset for this client\n",
    "        client_datasets.append(Subset(full_dataset, client_indices))\n",
    "\n",
    "    return client_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d6a55a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:05<00:00, 1703469.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 61773.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1333267.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2041419.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.ToTensor()\n",
    "full_dataset = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "\n",
    "client_datasets = create_client_datasets_from_split(full_dataset, train_data_statistics)\n",
    "# client_datasets = create_client_datasets_from_split(full_dataset, split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67922543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(LOGDIR)\n",
    "os.mkdir(f\"{LOGDIR}/clients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db385a",
   "metadata": {},
   "source": [
    "# BFL with IVON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93be2a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Round 1 ---\n",
      "Global Test Accuracy: 77.02%\n",
      "Global Test ECE: 0.42\n",
      "Global Test NLL: 1.21\n",
      "\n",
      "--- Round 2 ---\n",
      "Global Test Accuracy: 93.03%\n",
      "Global Test ECE: 0.03\n",
      "Global Test NLL: 0.22\n",
      "\n",
      "--- Round 3 ---\n",
      "Global Test Accuracy: 95.75%\n",
      "Global Test ECE: 0.01\n",
      "Global Test NLL: 0.13\n",
      "\n",
      "--- Round 4 ---\n",
      "Global Test Accuracy: 96.77%\n",
      "Global Test ECE: 0.01\n",
      "Global Test NLL: 0.10\n"
     ]
    }
   ],
   "source": [
    "# Global model\n",
    "global_model = CNN(input_dim, hidden_dims, output_dim=output_dim, input_channel=input_channel)\n",
    "global_model.to(DEVICE)\n",
    "global_optimizer = IVON_SAMP(global_model.parameters(), lr=LR, ess=1, weight_decay=REG, beta1=0.9, hess_init=HESS_INIT, cov=None)\n",
    "torch.save(global_optimizer.state_dict(), f\"{LOGDIR}/global_optimizer.pt\")\n",
    "\n",
    "for round in range(1, 5):\n",
    "    local_weights = []\n",
    "    covs = []\n",
    "    print(f\"\\n--- Round {round} ---\")\n",
    "\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        local_model = CNN(input_dim, hidden_dims, output_dim=output_dim, input_channel=input_channel)\n",
    "\n",
    "        # Initialize local model with global model\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        train_loader = DataLoader(client_datasets[i], batch_size=BATCH_SIZE, shuffle=True)\n",
    "        N = len(train_loader.dataset)\n",
    "        local_model.to(DEVICE)\n",
    "        optimizer = ivon.IVON(\n",
    "            local_model.parameters(),\n",
    "            lr=LR,\n",
    "            weight_decay=REG,\n",
    "            ess=N,\n",
    "            beta1=0.9,\n",
    "            hess_init=HESS_INIT\n",
    "            )\n",
    "        # If not the first round, set the covariance from the global model\n",
    "        if round > 1:\n",
    "            global_cov = global_optimizer.param_groups[0][\"cov\"]\n",
    "            global_hessian = (1/ (N * global_cov)) - REG\n",
    "            optimizer.param_groups[0][\"hess\"] = torch.from_numpy(global_hessian.astype(np.float32)).to(DEVICE)\n",
    "\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        local_w = train_ivon(local_model, train_loader, criterion, optimizer, EPOCHS, client_id=i)\n",
    "        optimizer.load_state_dict(torch.load(f\"{LOGDIR}/clients/client_{i}_optimizer.pt\"))\n",
    "        # Compute covariance\n",
    "        cov = 1 / (N * optimizer.state_dict()['param_groups'][0]['hess'] + optimizer.state_dict()['param_groups'][0]['weight_decay'])\n",
    "        local_weights.append(local_w) # collect the local models (means)\n",
    "        covs.append(cov) # collect the local covariances\n",
    "\n",
    "\n",
    "    # Average weights and covariances\n",
    "    update_method = 'eaa' # choose the update method\n",
    "    weights = [len(ds) for ds in client_datasets]\n",
    "    weights = np.array(weights) / sum(weights)\n",
    "    averaged_weights , average_cov  = aggregate_ivon(local_weights, covs, update_method, global_model, weights=weights)\n",
    "\n",
    "\n",
    "    # Update global model with averaged weights and the global optimizer with averaged covariance\n",
    "    global_model.load_state_dict(averaged_weights)\n",
    "    global_optimizer = IVON_SAMP(global_model.parameters(), lr=LR, ess=1, weight_decay=REG, beta1=0.9, hess_init=HESS_INIT, cov=average_cov) #the ess is set to be 1 to pass the assertion. In all the cases it won't be used\n",
    "    torch.save(global_optimizer.state_dict(), f\"{LOGDIR}/global_optimizer.pt\")\n",
    "\n",
    "\n",
    "    # Update global optimizer with averaged covariance\n",
    "    global_optimizer.load_state_dict(torch.load(f\"{LOGDIR}/global_optimizer.pt\", weights_only=False))\n",
    "    global_optimizer.param_groups[0][\"cov\"] = average_cov\n",
    "    torch.save(global_optimizer.state_dict(), f\"{LOGDIR}/global_optimizer.pt\")\n",
    "\n",
    "    # Evaluate on small test set\n",
    "    global_model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    test_loader = DataLoader(datasets.MNIST('.', train=False, transform=transform), batch_size=1000)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            x, target = x.to(DEVICE), target.to(DEVICE,dtype=torch.int64)\n",
    "            outs = []\n",
    "            if N_SAMPLE == 0: #predict with the mean\n",
    "                out = global_model(x)\n",
    "                out = F.softmax(out, 1)\n",
    "                outs.append(out)\n",
    "\n",
    "            else :\n",
    "                for _ in range(N_SAMPLE):\n",
    "                    with global_optimizer.sampled_params():\n",
    "                        out = global_model(x)\n",
    "                        out = F.softmax(out, 1)\n",
    "                        outs.append(out)\n",
    "\n",
    "            preds.append(torch.stack(outs).mean(0))\n",
    "            targets.append(F.one_hot(target, 10))\n",
    "    targets = torch.cat(targets)\n",
    "    preds = torch.cat(preds)\n",
    "\n",
    "    _acc = acc(preds, targets, minibatch=False)\n",
    "    _ece = ece(preds, targets, DEVICE, minibatch=False)\n",
    "    _nll = nll(preds, targets, minibatch=False)\n",
    "    print(f\"Global Test Accuracy: {_acc:.2f}%\")\n",
    "    print(f\"Global Test ECE: {_ece:.2f}\")\n",
    "    print(f\"Global Test NLL: {_nll:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e43eb8",
   "metadata": {},
   "source": [
    "# FedAVG with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "966f74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Round 1 ---\n",
      "Global Test Accuracy: 9.74%\n",
      "\n",
      "--- Round 2 ---\n",
      "Global Test Accuracy: 29.91%\n",
      "\n",
      "--- Round 3 ---\n",
      "Global Test Accuracy: 68.76%\n",
      "\n",
      "--- Round 4 ---\n",
      "Global Test Accuracy: 77.25%\n"
     ]
    }
   ],
   "source": [
    "# FedAVG with SGD\n",
    "\n",
    "# Parameters\n",
    "LR = 0.01\n",
    "\n",
    "\n",
    "# FedAvg: average model parameters\n",
    "def average_weights(w_list):\n",
    "    avg_w = {}\n",
    "    for key in w_list[0].keys():\n",
    "        avg_w[key] = sum(w[key] for w in w_list) / len(w_list)\n",
    "    return avg_w\n",
    "\n",
    "# Training function for local client\n",
    "def train(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for data, target in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "# Global model\n",
    "global_model = CNN(input_dim, hidden_dims, output_dim=output_dim, input_channel=input_channel)\n",
    "\n",
    "for round in range(1, 5):\n",
    "    local_weights = []\n",
    "    print(f\"\\n--- Round {round} ---\")\n",
    "\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        local_model = CNN(input_channel=input_channel, input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        train_loader = DataLoader(client_datasets[i], batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        optimizer = optim.SGD(local_model.parameters(), lr=LR)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        local_w = train(local_model, train_loader, criterion, optimizer, EPOCHS)\n",
    "        local_weights.append(local_w)\n",
    "\n",
    "    # Average weights and update global model\n",
    "    averaged_weights = average_weights(local_weights)\n",
    "    global_model.load_state_dict(averaged_weights)\n",
    "\n",
    "    # Evaluate on small test set\n",
    "    test_loader = DataLoader(datasets.MNIST('.', train=False, transform=transform), batch_size=1000)\n",
    "    global_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            pred = global_model(x).argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    print(f\"Global Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
